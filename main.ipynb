{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81011136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "import evaluate\n",
    "from utils.tools import get_parameter_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82367e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import joblib\n",
    "from typing import Optional\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "\n",
    "\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "146d9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    accelerate.utils.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    if torch.cuda.is_available() > 0:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97b365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"task_name\": \"long_term_forecast\",\n",
    "    \"is_training\": 1,\n",
    "    \"model_id\": \"test\",\n",
    "    \"model_comment\": \"none\",\n",
    "    \"model\": \"CPMLP\",\n",
    "    \"seed\": 2021,\n",
    "    \"charge_discharge_length\": 300,\n",
    "    \"dataset\": \"CALB\",\n",
    "    \"data\": \"BatteryLife\",\n",
    "    \"root_path\": \"./dataset/processed\",\n",
    "    \"data_path\": \"ETTh1.csv\",\n",
    "    \"features\": \"M\",\n",
    "    \"target\": \"OT\",\n",
    "    \"loader\": \"modal\",\n",
    "    \"freq\": \"h\",\n",
    "    \"checkpoints\": \"./checkpoints/\",\n",
    "    \"early_cycle_threshold\": 100,\n",
    "    \"seq_len\": 1,\n",
    "    \"pred_len\": 5,\n",
    "    \"label_len\": 48,\n",
    "    \"seasonal_patterns\": \"Monthly\",\n",
    "    \"enc_in\": 1,\n",
    "    \"dec_in\": 1,\n",
    "    \"c_out\": 1,\n",
    "    \"d_model\": 128,\n",
    "    \"n_heads\": 8,\n",
    "    \"lstm_layers\": 2,\n",
    "    \"e_layers\": 4,\n",
    "    \"d_layers\": 2,\n",
    "    \"d_ff\": 256,\n",
    "    \"moving_avg\": 25,\n",
    "    \"factor\": 1,\n",
    "    \"dropout\": 0.0,\n",
    "    \"embed\": \"timeF\",\n",
    "    \"activation\": \"relu\",\n",
    "    \"output_attention\": False,\n",
    "    \"patch_len\": 10,\n",
    "    \"stride\": 10,\n",
    "    \"patch_len2\": 10,\n",
    "    \"stride2\": 10,\n",
    "    \"prompt_domain\": 0,\n",
    "    \"output_num\": 1,\n",
    "    \"class_num\": 8,\n",
    "    \"weighted_loss\": False,\n",
    "    \"weighted_sampling\": False,\n",
    "    \"num_workers\": 1,\n",
    "    \"itr\": 1,\n",
    "    \"train_epochs\": 100,\n",
    "    \"least_epochs\": 5,\n",
    "    \"batch_size\": 16,\n",
    "    \"patience\": 5,\n",
    "    \"learning_rate\": 5e-05,\n",
    "    \"wd\": 0.0,\n",
    "    \"des\": \"test\",\n",
    "    \"loss\": \"MSE\",\n",
    "    \"lradj\": \"constant\",\n",
    "    \"pct_start\": 0.2,\n",
    "    \"use_amp\": False,\n",
    "    \"percent\": 100,\n",
    "    \"accumulation_steps\": 1,\n",
    "    \"mlp\": 0,\n",
    "    \"alpha1\": 0.15,\n",
    "    \"alpha2\": 0.1,\n",
    "}\n",
    "\n",
    "class AttrDict(dict):\n",
    "    \"\"\"A dictionary that allows for attribute-style access.\"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value\n",
    "\n",
    "args = AttrDict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ce7cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "nowtime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "set_seed(args.seed)\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "deepspeed_plugin = DeepSpeedPlugin(hf_ds_config=\"./ds_config_zero2_baseline.json\")\n",
    "accelerator = Accelerator(\n",
    "    kwargs_handlers=[ddp_kwargs],\n",
    "    deepspeed_plugin=deepspeed_plugin,\n",
    "    gradient_accumulation_steps=args.accumulation_steps,\n",
    ")\n",
    "logger: Optional[Logger] = None\n",
    "if accelerator.is_local_main_process:\n",
    "    logger = Logger(\"logs\", args.model + args.model_comment)\n",
    "    logger.log_hparams(vars(args))\n",
    "accelerator.print(args.__dict__)\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = \"{}_sl{}_lr{}_dm{}_nh{}_el{}_dl{}_df{}_lradj{}_dataset{}_loss{}_wd{}_wl{}_bs{}_s{}\".format(\n",
    "        args.model,\n",
    "        args.seq_len,\n",
    "        args.learning_rate,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.lradj,\n",
    "        args.dataset,\n",
    "        args.loss,\n",
    "        args.wd,\n",
    "        args.weighted_loss,\n",
    "        args.batch_size,\n",
    "        args.seed,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, drop_rate):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.in_linear = nn.Linear(in_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.out_linear = nn.Linear(hidden_dim, out_dim)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [B, *, in_dim]\n",
    "        '''\n",
    "        out = self.in_linear(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out_linear(out)\n",
    "        out = self.ln(self.dropout(out) + x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class CPMLP(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(CPMLP, self).__init__()\n",
    "        self.d_ff = configs.d_ff\n",
    "        self.d_model = configs.d_model\n",
    "        self.charge_discharge_length = configs.charge_discharge_length\n",
    "        self.early_cycle_threshold = configs.early_cycle_threshold\n",
    "        self.drop_rate = configs.dropout\n",
    "        self.e_layers = configs.e_layers\n",
    "        self.d_layers = configs.d_layers\n",
    "        self.intra_flatten = nn.Flatten(start_dim=2)\n",
    "        self.intra_embed = nn.Linear(self.charge_discharge_length*3, self.d_model)\n",
    "        self.intra_MLP = nn.ModuleList([MLPBlock(self.d_model, self.d_ff, self.d_model, self.drop_rate) for _ in range(configs.e_layers)])\n",
    "\n",
    "        self.inter_flatten = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1), \n",
    "            nn.Linear(self.early_cycle_threshold*self.d_model, self.d_model)\n",
    "        )\n",
    "        self.inter_MLP = nn.ModuleList([MLPBlock(self.d_model, self.d_ff, self.d_model, self.drop_rate) for _ in range(configs.d_layers)])\n",
    "        self.head_output = nn.Linear(self.d_model, 1)\n",
    "\n",
    "    def forward(self, cycle_curve_data, curve_attn_mask, return_embedding=False):\n",
    "        '''\n",
    "        cycle_curve_data: [B, early_cycle, fixed_len, num_var]\n",
    "        curve_attn_mask: [B, early_cycle]\n",
    "        '''\n",
    "        print(f\"input shape: {cycle_curve_data.shape}\")\n",
    "        cycle_curve_data = self.intra_flatten(cycle_curve_data) # [B, early_cycle, fixed_len * num_var]\n",
    "        print(f\"after flatten shape: {cycle_curve_data.shape}\")\n",
    "        cycle_curve_data = self.intra_embed(cycle_curve_data)\n",
    "        print(f\"after embed shape | encode input: {cycle_curve_data.shape}\")\n",
    "        for i in range(self.e_layers):\n",
    "            cycle_curve_data = self.intra_MLP[i](cycle_curve_data) # [B, early_cycle, d_model]\n",
    "        print(f\"after encode shape: {cycle_curve_data.shape}\")\n",
    "        cycle_curve_data = self.inter_flatten(cycle_curve_data) # [B, d_model]\n",
    "        for i in range(self.d_layers):\n",
    "            cycle_curve_data = self.inter_MLP[i](cycle_curve_data) # [B, d_model]\n",
    "        print(f\"after decode shape: {cycle_curve_data.shape}\")\n",
    "        preds = self.head_output(F.relu(cycle_curve_data))\n",
    "        print(f\"pred shape: {preds.shape}\")\n",
    "        if return_embedding:\n",
    "            return preds, cycle_curve_data\n",
    "        else:\n",
    "            return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbd872e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CPMLP(args).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbcf1a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training samples......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:04<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vali samples......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test samples......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.90it/s]\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(\n",
    "    args.checkpoints, setting + \"-\" + args.model_comment\n",
    ")  # unique checkpoint saving path\n",
    "\n",
    "accelerator.print(\"Loading training samples......\")\n",
    "train_data, train_loader = data_provider(\n",
    "    args, flag=\"train\"\n",
    ")\n",
    "label_scaler = train_data.return_label_scaler()\n",
    "life_class_scaler = train_data.return_life_class_scaler()\n",
    "accelerator.print(\"Loading vali samples......\")\n",
    "vali_data, vali_loader = data_provider(\n",
    "    args,\n",
    "    flag=\"val\",\n",
    "    label_scaler=label_scaler,\n",
    "    life_class_scaler=life_class_scaler,\n",
    ")\n",
    "accelerator.print(\"Loading test samples......\")\n",
    "test_data, test_loader = data_provider(\n",
    "    args,\n",
    "    flag=\"test\",\n",
    "    label_scaler=label_scaler,\n",
    "    life_class_scaler=life_class_scaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc753e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100, 3, 300])\n",
      "torch.Size([16, 100])\n",
      "torch.Size([16, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "input shape: torch.Size([16, 100, 3, 300])\n",
      "after flatten shape: torch.Size([16, 100, 900])\n",
      "after embed shape | encode input: torch.Size([16, 100, 128])\n",
      "after encode shape: torch.Size([16, 100, 128])\n",
      "after decode shape: torch.Size([16, 128])\n",
      "pred shape: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (\n",
    "        cycle_curve_data,\n",
    "        curve_attn_mask,\n",
    "        labels,\n",
    "        life_class,\n",
    "        scaled_life_class,\n",
    "        weights,\n",
    "        seen_unseen_ids,\n",
    "    ) in enumerate(train_loader):\n",
    "    print(cycle_curve_data.shape)\n",
    "    print(curve_attn_mask.shape)\n",
    "    print(labels.shape)\n",
    "    print(life_class.shape)\n",
    "    print(scaled_life_class.shape)\n",
    "    print(weights.shape)\n",
    "    print(seen_unseen_ids.shape)\n",
    "\n",
    "    outputs = model(cycle_curve_data, curve_attn_mask)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06440fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3f6b0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2515],\n",
       "        [ 0.7662],\n",
       "        [-1.2982],\n",
       "        [-1.2982],\n",
       "        [ 0.7793],\n",
       "        [-1.3696],\n",
       "        [-1.4034],\n",
       "        [-1.4053],\n",
       "        [-1.4053],\n",
       "        [ 0.7305],\n",
       "        [ 0.8751],\n",
       "        [ 0.7662],\n",
       "        [ 0.7155],\n",
       "        [ 0.9390],\n",
       "        [-1.4034],\n",
       "        [ 0.7343]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764cd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BatteryPackLife",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
